import email
import json
import re
import os
import uuid
from colorama import Fore, Style
import click

from util import extractURLs, cleanScreenshotsFolder
from email.header import decode_header
from screenshot import take_screenshot
from checks import getDomain, havingIP, haveAtSign, get_whois_info, domainAge, domainEnd, checkEnglish, prefixSuffix, tinyURL, checkLegitimate, getLength

urls = []
domains = []
summary = []


def parse_email(eml_file):
    global urls

    # read eml file
    with open(eml_file, 'rb') as file:
        msg = email.message_from_bytes(file.read())

    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            content_disposition = str(part.get("Content-Disposition"))
            if content_type == 'text/plain' and 'attachment' not in content_disposition:
                try:
                    body = part.get_payload(decode=True)
                    decoded_body = body.decode('utf-8')
                    extracted_urls = extractURLs(decoded_body)

                    for url in extracted_urls:
                        urls.append(url)
                except UnicodeDecodeError:
                    with open('error.log', 'a') as error_file:
                        error_file.write(f"{eml_file}\n")
                    pass

            elif content_type == 'text/html' and 'attachment' not in content_disposition:
                try:
                    html_body = part.get_payload(decode=True)
                    decoded_html_body = html_body.decode('utf-8')
                    extracted_urls = extractURLs(decoded_html_body)

                    for url in extracted_urls:
                        urls.append(url)
                except UnicodeDecodeError:
                    with open('error.log', 'a') as error_file:
                        error_file.write(f"{eml_file}\n")
                    pass

    else:
        body = msg.get_payload(decode=True)
        extracted_urls = extractURLs(body.decode('utf-8'))

        for url in extracted_urls:
            urls.append(url)


def updateSummary(url, key, value):
    for item in summary:
        if item['url'] == url:
            item['checks'][key] = value
            break


def updateSummaryWhois(url, value):
    for item in summary:
        if item['url'] == url:
            item['whois'] = value
            break


def updateScreenshot(url, value):
    for item in summary:
        if item['url'] == url:
            item['screenshot'] = value
            break


@click.command()
@click.option('--file', type=str, help='Specify the file to scan')
def main(file):

    if file is None:
        print(
            f"{Fore.RED}[ERROR] Please specify the file to scan {Style.RESET_ALL}")
        return

    print(
        f"{Fore.YELLOW}[DEBUG] Cleaning screenshots folder {Style.RESET_ALL}")
    cleanScreenshotsFolder()

    print(
        f"{Fore.YELLOW}[DEBUG] Parsing emali content {file} {Style.RESET_ALL}")
    
    parse_email(file)

    extracted_urls = []

    extracted_urls = list(dict.fromkeys(urls))
    domains = [getDomain(url) for url in extracted_urls]

    caseId = str(uuid.uuid4())

    for url in extracted_urls:
        data = {
            "caseId": caseId,
            "url": url,
            "screenshot": "",
            "domain": getDomain(url),
            "whois": {},
            "checks": {
                "ip_address_in_url": 0,
                "at_sign_in_url": 0,
                "not_eng_in_url": 0,
                "prefix_suffix": 0,
                "tiny_url": 0,
                "domain_age": 0,
                "domain_end": 0,
                "not_legitimate": 0,
                "length": 0,
                "screenshot": 0
            }
        }
        summary.append(data)

    print(
        f"{Fore.GREEN}[INFO] Found {len(extracted_urls)} URLs {Style.RESET_ALL}")
    print(
        f"{Fore.GREEN}[INFO] Extracted URLs: {extracted_urls} URLs {Style.RESET_ALL}")

    for url in extracted_urls:
        # take screenshot of url
        filename = str(uuid.uuid4()) + '.png'
        take_screenshot(url, 'screenshots/' + filename)
        updateScreenshot(url, 'screenshots/' + filename)

        # check for IP address in URL
        ips = havingIP(url)

        if ips == 1:
            print(
                f"{Fore.RED}[WARNING] URL {url} has an IP address {Style.RESET_ALL}")
            updateSummary(url, 'ip_address_in_url', 1)

        # check for @ in URL
        atsign = haveAtSign(url)

        if atsign == 1:
            print(
                f"{Fore.RED}[WARNING] URL {url} has an @ sign {Style.RESET_ALL}")
            updateSummary(url, 'at_sign_in_url', 1)

        # check for english in URL
        eng = checkEnglish

        if eng == 1:
            print(
                f"{Fore.RED}[WARNING] URL {url} is not in English {Style.RESET_ALL}")
            updateSummary(url, 'not_eng_in_url', 1)

        # check for prefix or suffix in URL
        pre = prefixSuffix(url)

        if pre == 1:
            print(
                f"{Fore.RED}[WARNING] URL {url} has prefix or suffix {Style.RESET_ALL}")
            updateSummary(url, 'prefix_suffix', 1)

        # check if url is a tiny url
        tiny = tinyURL(url)

        if tiny == 1:
            print(
                f"{Fore.RED}[WARNING] URL {url} is a tiny URL {Style.RESET_ALL}")
            updateSummary(url, 'tiny_url', 1)

        
    # get whois info for each domain
    for domain in domains:

        # check if url is legitimate
        legit = checkLegitimate(domain)

        # get whois info
        if legit == 1:
            print(
                f"{Fore.RED}[WARNING] Domain {domain} is not legitimate {Style.RESET_ALL}")
            updateSummary(url, 'not_legitimate', 1)
            print(
                f"{Fore.YELLOW}[DEBUG] Getting WHOIS info for domain {domain} {Style.RESET_ALL}")
            whois_response = get_whois_info(domain)

            if whois_response is not None:
                # print available whois info
                if whois_response.domain_name:
                    print(f"Domain name: {whois_response.domain_name}")
                if whois_response.registrar:
                    print(f"Registrar: {whois_response.registrar}")
                if whois_response.whois_server:
                    print(f"WHOIS server: {whois_response.whois_server}")
                if whois_response.creation_date:
                    print(f"Creation date: {whois_response.creation_date}")
                if whois_response.expiration_date:
                    print(f"Expiration date: {whois_response.expiration_date}")
                if whois_response.name_servers:
                    print(f"Name servers: {whois_response.name_servers}")
                if whois_response.emails:
                    print(f"Emails: {whois_response.emails}")
                if whois_response.org:
                    print(f"Organization: {whois_response.org}")
                if whois_response.address:
                    print(f"Address: {whois_response.address}")
                if whois_response.city:
                    print(f"City: {whois_response.city}")
                if whois_response.state:
                    print(f"State: {whois_response.state}")
                if whois_response.zipcode:
                    print(f"ZIP code: {whois_response.zipcode}")
                if whois_response.country:
                    print(f"Country: {whois_response.country}")

                # check if creation date and expiration date is available
                if whois_response.creation_date is None and whois_response.expiration_date is None:
                    # check domain age
                    age = domainAge(whois_response.creation_date,
                                    whois_response.expiration_date)
                    if age == 1:
                        print(
                            f"{Fore.RED}[WARNING] Domain {domain} age is less than 12 months {Style.RESET_ALL}")
                        updateSummary(url, 'domain_age', 1)

            updateSummaryWhois(url, whois_response)

            # check domain expiration date
            end = domainEnd(domain)
            if end == 1:
                print(
                    f"{Fore.RED}[WARNING] Domain {domain} has remaining only 6 months {Style.RESET_ALL}")
                updateSummary(url, 'domain_end', 1)

    # write a function to calculate a threat score for each url depending on the checks
    for item in summary:
        threat_score = 0
        for key, value in item['checks'].items():
            if value == 1:
                threat_score += 1
        item['threat_score'] = threat_score

    # if item has more than 3 checks, it is a phishing url
    for item in summary:
        if item['threat_score'] > 3:
            item['phishing'] = 1
        else:
            item['phishing'] = 0

    # print each url threat score and finally suspect phishing urls
    for item in summary:
        print(
            f"{Fore.MAGENTA}[NOTE] URL {item['url']} has a threat score of {item['threat_score']} {Style.RESET_ALL}")
        if item['phishing'] == 1:
            print(
                f"{Fore.RED}[ALERT!!!] URL {item['url']} is a phishing URL {Style.RESET_ALL}")

    # write summary into a json file
    filename = 'logs/' + caseId + '.json'
    with open(filename, 'w') as json_file:
        json.dump(summary, json_file, default=str)


if __name__ == "__main__":
    main()
